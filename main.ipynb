{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install Augmentor supervision opencv-python autodistill autodistill-yolov8 autodistill-sam-clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the necessary library to import for this code snippet.\n",
    "from Augmentor import Pipeline\n",
    "\n",
    "augment = Pipeline(\"./images\", \"augmented\") # This function defines a pipeline with input and output folders as arguments (output folder hasn't to exist necessarily, it is created as subfolder of input one).\n",
    "augment.flip_left_right(probability = 0.5)\n",
    "augment.flip_top_bottom(probability = 0.5)\n",
    "augment.zoom(probability = 0.5, min_factor = 1.1, max_factor = 2)\n",
    "augment.rotate(probability = 0.5, max_left_rotation = 5, max_right_rotation = 10)\n",
    "\n",
    "augment.sample(10000) # This method takes as input the number of images to generate and starts the augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mautodistill\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdetection\u001b[39;00m \u001b[39mimport\u001b[39;00m CaptionOntology\n\u001b[0;32m      5\u001b[0m                                                    \u001b[39m# Caption*           # Class**\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m annotate \u001b[39m=\u001b[39m SAMCLIP(ontology \u001b[39m=\u001b[39;49m CaptionOntology({\u001b[39m\"\u001b[39;49m\u001b[39mHuman fingernail.\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mfingernail\u001b[39;49m\u001b[39m\"\u001b[39;49m})) \u001b[39m# Dict can contain multiple \"caption-class\" pairs.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m annotate\u001b[39m.\u001b[39mlabel(\n\u001b[0;32m      9\u001b[0m   input_folder \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./images/augmented\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m# Insert all the training images here (folder path name can be changed).\u001b[39;00m\n\u001b[0;32m     10\u001b[0m   output_folder \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./dataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m# This folder is auto-created, so it's not necessary create it before (folder path name can be changed).\u001b[39;00m\n\u001b[0;32m     11\u001b[0m ) \u001b[39m# This method splits the given images in \"valid\" and \"train\" ones and auto-annotated all of them.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m model \u001b[39m=\u001b[39m YOLOv8(\u001b[39m\"\u001b[39m\u001b[39m./yolov8m-seg.pt\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m# This function downloads the pt YOLOv8 model file***\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\autodistill_sam_clip\\sam_clip.py:35\u001b[0m, in \u001b[0;36mSAMCLIP.__init__\u001b[1;34m(self, ontology)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, ontology: CaptionOntology):\n\u001b[0;32m     34\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39montology \u001b[39m=\u001b[39m ontology\n\u001b[1;32m---> 35\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msam_predictor \u001b[39m=\u001b[39m load_SAM()\n\u001b[0;32m     37\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mHOME\u001b[39m}\u001b[39;00m\u001b[39m/.cache/autodistill/clip\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     38\u001b[0m         os\u001b[39m.\u001b[39mmakedirs(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mHOME\u001b[39m}\u001b[39;00m\u001b[39m/.cache/autodistill/clip\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\autodistill_sam_clip\\helpers.py:81\u001b[0m, in \u001b[0;36mload_SAM\u001b[1;34m()\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39m# Download the file if it doesn't exist\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(SAM_CHECKPOINT_PATH):\n\u001b[1;32m---> 81\u001b[0m     urllib\u001b[39m.\u001b[39;49mrequest\u001b[39m.\u001b[39;49murlretrieve(url, SAM_CHECKPOINT_PATH)\n\u001b[0;32m     83\u001b[0m SAM_ENCODER_VERSION \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvit_h\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     85\u001b[0m sam \u001b[39m=\u001b[39m sam_model_registry[SAM_ENCODER_VERSION](checkpoint\u001b[39m=\u001b[39mSAM_CHECKPOINT_PATH)\u001b[39m.\u001b[39mto(\n\u001b[0;32m     86\u001b[0m     device\u001b[39m=\u001b[39mDEVICE\n\u001b[0;32m     87\u001b[0m )\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\urllib\\request.py:268\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    265\u001b[0m     reporthook(blocknum, bs, size)\n\u001b[0;32m    267\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m     block \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mread(bs)\n\u001b[0;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m block:\n\u001b[0;32m    270\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 463\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    464\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    465\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    502\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[0;32m    504\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[0;32m    509\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1240\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1241\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1099\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1100\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# These are the necessary libraries to import for this code snippet and the next two ones.\n",
    "from autodistill_yolov8 import YOLOv8\n",
    "from autodistill_sam_clip import SAMCLIP\n",
    "from autodistill.detection import CaptionOntology\n",
    "                                               # Caption*           # Class**\n",
    "annotate = SAMCLIP(ontology = CaptionOntology({\"Human fingernail.\": \"fingernail\"})) # Dict can contain multiple \"caption-class\" pairs.\n",
    "\n",
    "annotate.label(\n",
    "  input_folder = \"./images/augmented\", # Insert all the training images here (folder path name can be changed).\n",
    "  output_folder = \"./dataset\" # This folder is auto-created, so it's not necessary create it before (folder path name can be changed).\n",
    ") # This method splits the given images in \"valid\" and \"train\" ones and auto-annotated all of them.\n",
    "\n",
    "model = YOLOv8(\"./yolov8m-seg.pt\") # This function downloads the pt YOLOv8 model file***\n",
    "\n",
    "\"\"\"\n",
    "Here are 2 of the key features of SAM (Segment Anything Model, an AI computer vision model created by Meta):\n",
    "∙ It can identify and segment objects in an image thanks to text captions;\n",
    "∙ It is able to learn the relationships between words and objects.\n",
    "SAMCLIP function takes from this large model only the necessary images to learn how to annotate the given ones.\n",
    "\n",
    "* Caption is a prompt: it should be as descriptive as possible, concise and grammatically correct (that's why the capital letter and the dot).\n",
    "** Class is a label that is used for the respective caption in the generated annotations: label should be the name of the object.\n",
    "*** Available instance segmentation options (from smallest, but less accurate to largest, but more accurate):\n",
    "∙ yolov8n-seg.pt -> nano (3.4M parameters)\n",
    "∙ yolov8s-seg.pt -> small (11.8M parameters)\n",
    "∙ yolov8m-seg.pt -> medium (27.3M parameters)\n",
    "∙ yolov8l-seg.pt -> large (46.0M parameters)\n",
    "∙ yolov8x-seg.pt -> extra-large (71.8M parameters)\n",
    "\n",
    "The key concept of this code is to use a bigger slower model to train smaller faster portable ones.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(\"./dataset/data.yaml\") # This method trains a model based on auto-annotated images (data.yaml is the default configuration file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(\"./test.jpg\")[0] # This method performs an instance segmentation on the input image to predict where the objects are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cv2 import imread, imwrite\n",
    "from supervision import Detections, MaskAnnotator\n",
    "\n",
    "imwrite(\"./test.jpg\",\n",
    "        MaskAnnotator().annotate(\n",
    "            scene = imread(\"./test.jpg\"),\n",
    "            detections = Detections.from_yolov8(predict),\n",
    "            opacity = 1.0\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
