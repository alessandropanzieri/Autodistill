{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install autodistill autodistill-grounded-sam autodistill-yolov8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the necessary libraries\n",
    "from autodistill_yolov8 import YOLOv8\n",
    "from autodistill_grounded_sam import GroundedSAM\n",
    "from autodistill.detection import CaptionOntology\n",
    "                                                    # caption*           # class**\n",
    "annotator = GroundedSAM(ontology = CaptionOntology({\"Human fingernail.\": \"fingernail\"})) # dict can contain multiple \"caption-class\" pairs\n",
    "\n",
    "annotator.label(\n",
    "  input_folder = \"./images\", # insert all the training images here (name can be changed)\n",
    "  output_folder = \"./dataset\" # this folder will be auto-generated, so it's not necessary create it before (name can be changed)\n",
    ") # this method splits the given images in \"valid\" and \"train\" ones and auto-annotated all\n",
    "\n",
    "model = YOLOv8(\"./yolov8m-seg.pt\") # this function auto-downloads the YOLOv8 pt model file ***\n",
    "\n",
    "\"\"\"\n",
    "here are 2 of the key features of SAM (Segment Anything Model, an AI computer vision model created by Meta):\n",
    "∙ it can identify and segment objects in an image thanks to text captions\n",
    "∙ it is able to learn the relationships between words and objects\n",
    "GroundedSAM function takes from this large model only the necessary images to learn how to annotate the given ones\n",
    "* caption is a prompt: it must be as descriptive as possible, concise and grammatically correct (that's why the capital letter and the dot)\n",
    "** class is a label that will be use for the respective caption in the generated annotations: label should be the name of the object\n",
    "*** available instance segmentation options (from smallest, but less accurate to largest, but more accurate):\n",
    "∙ yolov8n-seg.pt -> nano (3.4M parameters)\n",
    "∙ yolov8s-seg.pt -> small (11.8M parameters)\n",
    "∙ yolov8m-seg.pt -> medium (27.3M parameters)\n",
    "∙ yolov8l-seg.pt -> large (46.0M parameters)\n",
    "∙ yolov8x-seg.pt -> extra-large (71.8M parameters)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(\"./dataset/data.yaml\") # this method trains a model based on the auto-annotated images (data.yaml is default configuration file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(\"test.jpg\") # this method performs an instance segmentation on the input image to predict where the objects are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
