{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install Augmentor supervision opencv-python autodistill autodistill-yolov8 autodistill-grounded-sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the necessary library to import for this code snippet.\n",
    "from Augmentor import Pipeline\n",
    "\n",
    "augment = Pipeline(\"./images\", \"augmented\") # This function defines a pipeline with input and output folders as arguments (output folder hasn't to exist necessarily, it is created as subfolder of input one).\n",
    "augment.flip_left_right(probability = 0.5)\n",
    "augment.flip_top_bottom(probability = 0.5)\n",
    "augment.zoom(probability = 0.5, min_factor = 1.1, max_factor = 2)\n",
    "augment.rotate(probability = 0.5, max_left_rotation = 5, max_right_rotation = 10)\n",
    "\n",
    "augment.sample(10000) # This method takes as input the number of images to generate and starts the augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the necessary libraries to import for this code snippet and the next two ones.\n",
    "from autodistill_yolov8 import YOLOv8\n",
    "from autodistill_grounded_sam import GroundedSAM\n",
    "from autodistill.detection import CaptionOntology\n",
    "                                                   # Caption*           # Class**\n",
    "annotate = GroundedSAM(ontology = CaptionOntology({\"Human fingernail.\": \"fingernail\"})) # Dict can contain multiple \"caption-class\" pairs.\n",
    "\n",
    "annotate.label(\n",
    "  input_folder = \"./images/augmented\", # Insert all the training images here (folder path name can be changed).\n",
    "  output_folder = \"./dataset\" # This folder is auto-created, so it's not necessary create it before (folder path name can be changed).\n",
    ") # This method splits the given images in \"valid\" and \"train\" ones and auto-annotated all of them.\n",
    "\n",
    "model = YOLOv8(\"./yolov8m-seg.pt\") # This function downloads the pt YOLOv8 model file***\n",
    "\"\"\"\n",
    "Here are 2 of the key features of SAM (Segment Anything Model, an AI computer vision model created by Meta):\n",
    "∙ It can identify and segment objects in an image thanks to text captions;\n",
    "∙ It is able to learn the relationships between words and objects.\n",
    "GroundedSAM function takes from this large model only the necessary images to learn how to annotate the given ones.\n",
    "\n",
    "* Caption is a prompt: it should be as descriptive as possible, concise and grammatically correct (that's why the capital letter and the dot).\n",
    "** Class is a label that is used for the respective caption in the generated annotations: label should be the name of the object.\n",
    "*** Available instance segmentation options (from smallest, but less accurate to largest, but more accurate):\n",
    "∙ yolov8n-seg.pt -> nano (3.4M parameters)\n",
    "∙ yolov8s-seg.pt -> small (11.8M parameters)\n",
    "∙ yolov8m-seg.pt -> medium (27.3M parameters)\n",
    "∙ yolov8l-seg.pt -> large (46.0M parameters)\n",
    "∙ yolov8x-seg.pt -> extra-large (71.8M parameters)\n",
    "\n",
    "The key concept of this code is to use a bigger slower model to train smaller faster portable ones.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(\"./dataset/data.yaml\") # This method trains a model based on auto-annotated images (data.yaml is the default configuration file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(\"./test.jpg\")[0] # This method performs an instance segmentation on the input image to predict where the objects are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the necessary libraries to import for this code snippet.\n",
    "from cv2 import imread, imwrite\n",
    "from supervision import Detections, MaskAnnotator\n",
    "\n",
    "imwrite(\"./test.jpg\",\n",
    "        MaskAnnotator().annotate(\n",
    "            scene = imread(\"./test.jpg\"),\n",
    "            detections = Detections.from_yolov8(predict),\n",
    "            opacity = 1.0\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
