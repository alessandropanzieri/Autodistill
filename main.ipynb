{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install autodistill autodistill-yolov8 autodistill-grounded-sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the necessary libraries to import\n",
    "from autodistill_yolov8 import YOLOv8\n",
    "from autodistill_grounded_sam import GroundedSAM\n",
    "from autodistill.detection import CaptionOntology\n",
    "                                                    # Caption*           # Class**\n",
    "annotator = GroundedSAM(ontology = CaptionOntology({\"Human fingernail.\": \"fingernail\"})) # Dict can contain multiple \"caption-class\" pairs.\n",
    "\n",
    "annotator.label(\n",
    "  input_folder = \"./images\", # Insert all the training images here (folder path name can be changed).\n",
    "  output_folder = \"./dataset\" # This folder is auto-generated, so it's not necessary create it before (folder path name can be changed).\n",
    ") # This method splits the given images in \"valid\" and \"train\" ones and auto-annotated all of them.\n",
    "\n",
    "model = YOLOv8(\"./yolov8m-seg.pt\") # This function downloads the pt YOLOv8 model file***\n",
    "\n",
    "\"\"\"\n",
    "Here are 2 of the key features of SAM (Segment Anything Model, an AI computer vision model created by Meta):\n",
    "∙ It can identify and segment objects in an image thanks to text captions;\n",
    "∙ It is able to learn the relationships between words and objects.\n",
    "GroundedSAM function takes from this large model only the necessary images to learn how to annotate the given ones.\n",
    "\n",
    "* Caption is a prompt: it should be as descriptive as possible, concise and grammatically correct (that's why the capital letter and the dot).\n",
    "** Class is a label that is used for the respective caption in the generated annotations: label should be the name of the object.\n",
    "*** Available instance segmentation options (from smallest, but less accurate to largest, but more accurate):\n",
    "∙ yolov8n-seg.pt -> nano (3.4M parameters)\n",
    "∙ yolov8s-seg.pt -> small (11.8M parameters)\n",
    "∙ yolov8m-seg.pt -> medium (27.3M parameters)\n",
    "∙ yolov8l-seg.pt -> large (46.0M parameters)\n",
    "∙ yolov8x-seg.pt -> extra-large (71.8M parameters)\n",
    "\n",
    "The key concept of this code is to use bigger slower models to train smaller faster portable ones.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(\"./dataset/data.yaml\") # This method trains a model based on the auto-annotated images (data.yaml is the default configuration file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(\"test.jpg\") # This method performs an instance segmentation on the input image to predict where the objects are."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
